# Chapter 2

## Multi-armed Bandits

features of rl:

​	It uses information that evaluates the actions taken rather than instructs by giving correct actions.





active exploration

evaluate feedback: how good the action taken was

Instructive feedback: the correct action to taken.

**distinct feedbacks**

* evaluative feedback depends entirely on the action taken
* instructive feedback which is independent of the action taken



### 2.1 k-armed Bandits



Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levels.



Each of the k actions has an expected or mean reward given that the action is selected.

**The value of the action**

A<sub>t</sub>: **action for the time step t**

R<sub>t</sub>: **corresponding reward**

q<sub>*</sub>(a): **the expected reward when taking the action a**

Q<sub>t</sub>(a): **the estimated value of the action a at time step t**

q<sub>*</sub>(a)=E(R<sub>t</sub>|A<sub>t</sub>=a)



If we do not know the expected reward:

would like the estimated value Q<sub>t</sub>(a) to be q<sub>*</sub>(a)



**Greedy action**: At any step there is at least one action whose estimated value is greatest, you choose it.

**Exploiting**: choosing Greedy actions

**Exploring**: Choosing non-greedy actions

**Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.**

Balance!!!



### 2.2 Action-value Methods



**sum of rewards when a taken prior to t** and **number of times a taken prior to t**
$$
\sum_{i=1}^n R_i *Times(A_i=1) \div \sum_{i=1}^n Times(A_i=1)
$$


***sample-average Method* ** for estimating action values because each estimate is an average of the sample of relevant rewards.



**Action selection rule**

* gready: exploits current knowledge to maximize immediate reward
  $$
  A_t= argmax_a Q_t(a)
  $$
  
* $$
  \epsilon-greedy method
  $$

  behave greedily most of the time

  but with small probability P,select randomly from among all the actions with equal probability, independent of the action-value estimates
  $$
  P=\epsilon
  $$
  

  **Advantage**: in the limit as the number of steps increases, every action will be sampled an infinite number of times, which will ensure that all Q<sub>t</sub>(a) converge to q<sub>*</sub>(a)

  

### 2.3 The 10-armed Testbed

![image-20210222231615651](强化学习.assets/image-20210222231615651.png)



![image-20210222231659510](强化学习.assets/image-20210222231659510.png)



**The advantage of epsilon-greedy methods depends on the task**

With noisier rewards it takes more exploration to find the optimal action, epsilon-greedy methods should fare even better relative to the greedy method.

If the reward variances were zero, Then the greed method would know the true value of each action after trying it once.



Even in the deterministic case there is a large advantage to exploring if we weaken some of the other assumption. For example: the bandit task were non-stationary that is the true values of the actions changed over time.





### 2.4 Incremental Implementation

Efficient Manner: in particular, constant memory and constant per-time-step computation



R<sub>i</sub>: the reward after the i_th selection of this action

Q<sub>n</sub>: the estimate of its action value after it has been selected n-1 times
$$
Q_n=\frac{R_1+R_2+...+R_n}{n-1}
$$


But: **it would maintain a record of all the rewards and cost lots of computation and memory**


$$
Q_{n+1}=\frac{1}{n}\sum_{i=1}^nR_i
\\Q_{n+1}=\frac{1}{n}(R_n+\sum_{i=1}^{n-1}R_i)
\\Q_{n+1}=\frac{1}{n}(R_n+(n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i)
\\Q_{n+1}=\frac{1}{n}(R_n+(n-1)Q_{n})
\\Q_{n+1}=Q_{n}+\frac{1}{n}(R_n-Q_n)
$$




which will use small computation and memory



**Update Rule**
$$
NewEstimate=OldEstimate + StepSize[Target-OldEstimate]
$$

$$
[Target-OldEstimate] : error
$$


**A simple bandit algorithm**

Initialize for a =1 to k

​	Q(a)=0

​	N(a)=0

Loop forever

​	A use epsilon-greedy algorithm

​	R=bandit(A)  (the reward for action A)

​	N(A)=N(A)+1

​	Q(A)=Q(A)+[R-Q(A)]/N(A)

![image-20210222235034177](强化学习.assets/image-20210222235034177.png)

### 2.5 Tracking a Non-stationary Problem



We often encounter rl problems that are effectively non-stationary.



**ways**: change 1/n to a constant step-size parameter
$$
\\Q_{n+1}=Q_{n}+\alpha(R_n-Q_n)
$$

$$
\\Q_{n+1}=Q_{n}+\alpha(R_n-Q_n)
\\Q_{n+1}=\alpha R_{n}+(1-\alpha) Q_n
\\Q_{n+1}=\alpha R_{n}+(1-\alpha)(\alpha R_{n-1}+(1-\alpha) Q_{n-1})
\\Q_{n+1}=\alpha R_{n}+\alpha(1-\alpha) R_{n-1}+(1-\alpha)^2 Q_{n-1}
\\Q_{n+1}=(1-\alpha)^n Q_{1}+\sum_{i=1}^n\alpha(1-\alpha)^{n-i} R_{i}
$$
**Conditions** can be met by 1/n

<img src="强化学习.assets/image-20210222235106073.png" alt="image-20210222235106073" style="zoom:50%;" />

* the steps are large enough to eventually overcome any initial conditions or random fluctuations
* eventually the steps become small enough to ensure convergence **( second can not be met by alpha)**


$$
\alpha_n(a)=\frac{1}{n}
$$

$$
\alpha_n(a)=\alpha(1-\alpha)^{n-i}
$$



### 2.6 Optimistic Initial Values

All the methods above are *biased* by the initial action-value estimates



The kind of bias is usually not a problem and can sometimes be very helpful

* downside: the initial estimates become a set of parameters that must be picked by users
* upside: provide an easy way to supply some prior knowledge about what level of rewards can be expected



**Initial action values can also be used as a simple way to encourage exploration**

![image-20210222235909038](强化学习.assets/image-20210222235909038.png)



### 2.7 Upper-Confidence-Bound Action selection



epsilon-greedy

* forces the non-greedy actions to be tried
* no preference for those that are nearly greedy or particularly uncertain.

Improvement

* select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates





***UCB (Upper Confidence Bound)***
$$
A_t=argmax_a[A_t(a)+c\sqrt\frac{ln t}{N_t(a)}
$$
ln(t): the natural logarithm of t

N<sub>t</sub>(a): the number of times that action a has been selected prior to time t

c: controls the degree of exploration

![image-20210223000826251](强化学习.assets/image-20210223000826251.png)



### 2.8 Gradient Bandit Algorithms

Above:

* estimate action values and use the estimations to select actions



Now:

**The numerical preference for each action a : H<sub>t</sub>(a)**

* if we add 1000 to all the action preferences there is no effect on the action probabilities.


$$
P(A_t=a)=\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}=\pi_t(a)
$$




probability of taking action a at time t
$$
\pi_t(a)
$$


Stochastic gradient ascent

on each step, after selecting action A<sub>t</sub> and receive reward R<sub>t</sub> . the action preferences are updated 
$$
H_{t+1}(A_t)=^{update}H_t(A_t)+\alpha(R_t-R_{t}^{average})(1-\pi_t(A_t))
\\H_{t+1}(a)=^{update}H_t(a)-\alpha(R_t-R_{t}^{average})\pi_t(a)   \space for \space a\not=A_t
$$


the average of all the rewards serves as baseline with which the reward is compared.

If the reward is higher than the baseline, then the probability of taking A<sub>t</sub> in the future is increased, and if the reward is below baseline, then probability is decreased.

![image-20210223104816606](强化学习.assets/image-20210223104816606.png)



#### the Bandit Gradient Algorithm as Stochastic Gradient Ascent.



In exact gradient ascent, each preference H<sub>t</sub>(a) would be incremented proportional to the increment's effect on performance:
$$
H_{t+1}(a)=H_t(a)+\alpha\frac{\part{E(R_t)}}{\part{H_t(a)}}

\\ E[R_t]=\sum_{x}\pi_t(x)q_*(x)
$$


We do not know the q<sub>*</sub>(x)


$$
\frac{\part{E(R_t)}}{\part{H_t(a)}}
=\frac{\part{}}{\part{H_t(a)}}[\sum_{x}\pi_t(x)q_*(x)]
=\sum_{x}\frac{\part{\pi_t(x)}}{\part{H_t(a)}}q_*(x)
=\sum_{x}\frac{\part{\pi_t(x)}}{\part{H_t(a)}}[q_*(x)-B_t]
$$




B<sub>t</sub> is the baseline

we can include a baseline here without changing the equality because the gradient sums to zero over all actions.
$$
\frac{\part{E(R_t)}}{\part{H_t(a)}}
\\=\sum_{x}\pi_t(x)\frac{\part{\pi_t(x)}}{\part{H_t(a)}}[q_*(x)-B_t]/\pi_t(x)
\\=E[(q_*(A_t)-B_t)\frac{\part{\pi_t(A_t)}}{\part{H_t(a)}}/\pi_t(A_t)]
\\=E[(R_t-R_{t}^{average})\frac{\part{\pi_t(A_t)}}{\part{H_t(a)}}/\pi_t(A_t)]
$$


Soft max 

![image-20210223110630662](强化学习.assets/image-20210223110630662.png)

![image-20210223110744590](强化学习.assets/image-20210223110744590.png)



So we can get
$$
H_{t+1}(A_t)=^{update}H_t(A_t)+\alpha(R_t-R_{t}^{average})(1-\pi_t(A_t))
\\H_{t+1}(a)=^{update}H_t(a)-\alpha(R_t-R_{t}^{average})\pi_t(a)   \space for \space a\not=A_t
$$


**the choice of the baseline does not affect the expected update of the algorithm, but it does affect the variance of the update and thus the rate of convergence**





### 2.9 Associative Search

Non-associative tasks

* there is no need to associate different actions with different situations



More than one situation

* policy: a mapping from situation to the actions that are best in those situations



**full reinforcement learning problem**: If actions are allowed to affect the next situation as well as the reward



### 2.10 Summary

![image-20210223111649343](强化学习.assets/image-20210223111649343.png)

# Chapter 3 

## Finite Markov Decision Processes



* evaluative feedback
* choosing  different actions in different situations

MDPS

* actions influence not just immediate rewards but also subsequent situations or states
* involve delayed reward and trade-off immediate and delayed reward



### 3.1 The Agent-Environment Interface



*agent*: the learner and decision maker

*environment*: the thing the agent interacts with, comprising everything outside the agent

![image-20210223113052220](强化学习.assets/image-20210223113052220.png)

*trajectory*
$$
S_0,A_0,R_1
\\S_1,A_1,R_2
\\S_2,A_2,R_3
\\....
$$


Finite MDP

* the random variables R<sub>t</sub> and S<sub>t</sub> have defined discrete probability distributions dependent only on the **preceding** state and action

$$
p(s',r |s,a)=Pr(S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a)
$$


$$
\sum_{s'\in S}\sum_{r\in R}p(s',r|s,a)=1

$$


Expected rewards for state-action pairs:
$$
r(s,a)=E(R_t|S_{t-1}=s,A_{t-1}=a)=\sum_{r\in R}r\sum_{s' \in S}p(s',r|s,a)
$$


Expected rewards for state-action-next state pairs
$$
r(s,a,s')=E(R_t|S_{t-1}=s,A_{t-1}=a,S_t=s')=\sum_{r\in R}r \frac{p(s',r|s,a)}{p(s'|s,a)}
$$
![image-20210223120100240](强化学习.assets/image-20210223120100240.png)



### Goals and Rewards

The agent's goal is to maximize the total amount of reward it receives

This means maximizing not immediate reward, but cumulative reward in the long run.

**The reward hypothesis**

* that all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal(reward)



### Returns and Episodes

*The cumulative reward*

*expected return*
$$
G_t=R_{t+1}+R_{t+2}+....+R_T
$$




*discounting*
$$
\begin{aligned}
G_t
&=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+....\\
&=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}
\end{aligned}
$$

$$
\begin{aligned}
G_t
&=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+....\\
& = R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+...)  \\
&=R_{t+1}+\gamma G_{t+1}
\end{aligned}
$$


### 3.4 Unified Notation for Episodic and Continuing Tasks





tasks

* a sequence of separate episodic tasks
* continuing tasks



S<sub>t,i</sub> : the state representation at time of episode i



![image-20210224011606501](强化学习.assets/image-20210224011606501.png)

### 3.5 Policies and Value Functions





**value function**: function of states that estimate *how good* it is for the agent to be in a given state(expected return)



value functions are defined with respect to particular ways of acting called policies

policy: a map from states to probabilities of selecting each possible action.
$$
\pi(A_t=a|S_t=s)
$$
means the probability that if S<sub>t</sub>=s then  A<sub>t</sub>=a





action value function for policy 
$$
q_\pi(s,a)=E_\pi(G_t|S_t=s,A_t=a)=E_\pi(\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t=s,A_t=a)
$$




Monte Carlo methods: involve averaging over many random samples of actual returns





**Bellman Equation**: a relationship between the value of a state and the values of its successor states


$$
\begin{aligned}
v_\pi(s)
&=E_\pi(G_t|S_t=s)
\\&=E_\pi(R_{t+1}+\gamma G_t |S_t=s)
\\&= \sum_a\pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)\left[r+\gamma E_\pi(G_t|S_{t+1}=s')\right]
\\&=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
\end{aligned}
$$
<img src="强化学习.assets/image-20210224013254810.png" alt="image-20210224013254810" style="zoom:33%;" />



The value of the start state must be equal to the (discounted) value of the expected next state, plus the reward expected along the way



### 3.6 Optimal Policies and Optimal Value Functions



A policy is defined to be better than or equal to  another policy if its expected return is greater than or equal to than of another for all states.
$$
\pi>\pi' 
\\if \space and \space only \space if
\\v_\pi(s)>v_{\pi'}(s)
\\for \space all \space s\in S
$$






There is always at least one policy that is better than or equal to all other policies.
$$
v_*(s)=max_\pi v_\pi(s)
\\ for \space all \space s\in S
$$

$$
q_*(s,a)=max_\pi(s,a)
\\
q_*(s,a)=E(R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a)
$$






**Bellman optimality equation**: the value of a state under an optimal policy must equal the expected return for best action from the state.
$$
\begin{aligned}
v_*(s)
&=max_{a\in A(s)}q_{\pi_*}(s,a)
\\
&=max_a E_{\pi_*}[G_t|S_t=s,A_t=a]
\\
&=max_aE_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a]
\\
&=max_a E_[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]
\\
&=max_a \sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]
\end{aligned}
$$



$$
\begin{aligned}
q_*(s,a)
&=E[R_{t+1}+\gamma max_{a'}q_*(S_{t+1},a'|S_t=s,A_t=a)]
\\
&=\sum_{s',r}p(s',r|s,a)[r+\gamma q_*(s',a')]
\end{aligned}
$$
<img src="强化学习.assets/image-20210224015606757.png" alt="image-20210224015606757" style="zoom:40%;" />

if we have optimal value function, we will easily get an optimal policy.

Optimal value function: if one uses it to evaluate the short-term consequences of action, then a greedy policy is actually optimal in the long-term sense. because it already takes  into account the reward consequences of all possible future behavior.

![image-20210224020247853](强化学习.assets/image-20210224020247853.png)





<img src="强化学习.assets/image-20210224020420470.png" alt="image-20210224020420470" style="zoom:40%;" />







### 3.7 Optimality and Approximation



A critical aspect of the problem facing the agent is always the computational power available to it, in particular, the amount of computation it can perform in a single time step.



Memory

A large amount of memory is often required to build up approximations of value functions, policies and models.

Tabular Methods state(or state-action pair)





probability to select suboptimal actions for them has little impact on the amount of reward the agent receives.





## 3.8 Summary



# Chapter 4

## Dynamic Programming





Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically.



The key of DP:

* use of value functions to organize the structure the search for good policies.


$$
\begin{aligned}
v_*(s)
&=max_{a\in A(s)}q_{\pi_*}(s,a)
\\
&=max_a E_{\pi_*}[G_t|S_t=s,A_t=a]
\\
&=max_aE_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a]
\\
&=max_a E_[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]
\\
&=max_a \sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]
\end{aligned}
$$



$$
\begin{aligned}
q_*(s,a)
&=E[R_{t+1}+\gamma max_{a'}q_*(S_{t+1},a'|S_t=s,A_t=a)]
\\
&=\sum_{s',r}p(s',r|s,a)[r+\gamma q_*(s',a')]
\end{aligned}
$$

### 4.1 policy Evaluation

the probability of taking action a in state s under policy 
$$
\begin{aligned}
v_\pi(s)
&=E_\pi(G_t|S_t=s)
\\&=E_\pi(R_{t+1}+\gamma G_t |S_t=s)
\\&= \sum_a\pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)\left[r+\gamma E_\pi(G_t|S_{t+1}=s')\right]
\\&=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
\end{aligned}
$$


change it to **iterative solution methods**
$$
\begin{aligned}
v_{k+1}(s)
&=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_k(s')]
\end{aligned}
$$


![image-20210224022630778](强化学习.assets/image-20210224022630778.png)



### 4.2 Policy Improvement


$$
q_\pi(s,\pi'(s))\geq v_\pi(s)
$$
for all states
$$
v_{\pi'}(s)\geq v_\pi(s)
$$

$$
\begin{aligned}
v_\pi(s)
&\leq q_\pi(s,\pi'(s))
\\
&=E[R_{t+1}+\gamma v_\pi(S_{t+1})| S_t=s,A_t=\pi'(s)]
\\
&\leq E_{\pi'}[R_{t+1}+\gamma q_\pi(S_{t+1},\pi'(S_{t+1})|S_t=s]
\\
&=E_{\pi'}[R_{t+1}+\gamma E_{\pi'}[R_{t+2}+\gamma v_\pi(S_{t+2})|S_{t+1},A_{t+1}=\pi'(S_{t+1})]|S_t=s]
\\
&\leq E_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2v_\pi(S_{t+2})|S_t=s]
\\
&......
\\
&\leq E_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+....|S_t=s]
\\
&=v_{\pi'}(s)
\end{aligned}
$$


### policy Iteration


$$
\pi_0 \rightarrow^E v_{\pi_0}\rightarrow^{improve}\pi_1\rightarrow^E v_{\pi_1}\rightarrow^{improve}\pi_2\rightarrow^E ...\rightarrow^{improve}\pi_*\rightarrow^E v_*
$$


![image-20210224030310106](强化学习.assets/image-20210224030310106.png)



### 4.4 Value Iteration

![image-20210224030857686](强化学习.assets/image-20210224030857686.png)



<img src="强化学习.assets/image-20210224031018556.png" alt="image-20210224031018556" style="zoom:50%;" />

### 4.5 Asynchronous Dynamic Programming



### 4.6 Generalized Policy Iteration

* policy evaluation: make the value function consistent with the current policy
* policy improvement: make the policy greedy with respect to the current value function

<img src="强化学习.assets/image-20210224032824044.png" alt="image-20210224032824044" style="zoom:25%;" />





Each process drives the value function or policy toward one of the lines representing a solution to one of the two goals. The goals interact because the two line are not orthogonal. Driving directly toward one goal causes some movement away from the other goal.

<img src="强化学习.assets/image-20210224033126249.png" alt="image-20210224033126249" style="zoom:50%;" />



### 4.7 Efficiency of Dynamic Programming







### 4.8 Summary

* policy evaluation
  * iterative computation of value functions for a given policy
* policy improvement 
  * the computation of an improved policy given the value function for that policy
* policy iteration
* value iteration
* Expected update
  * closely related to Bellman equation
* GPI
  * two interacting processes revolving around an approximate policy and an approximate value function.
* DP: update estimates of the values of states based on estimates of the values of successor state.(*bootstrapping*)





# Chapter 5

## Monte Carlo Methods

* do not assume complete knowledge of the environment
* require only experience
* based on averaging sample returns
* assume: experience is divided into episodes. all episodes eventually terminate
* in episode-by-episode sense , but not in step-by-step(online) sense
* handle non-stationary : GPI



### 5.1 Monte Carlo Prediction

*return*

* expected cumulative future discounted reward

![image-20210224035001955](强化学习.assets/image-20210224035001955.png)


$$
\begin{aligned}
&Input: a \space policy \space \pi \space to \space to \space be \space evaluated
\\
&Initialize:
\\&\space\space\space\space\space\space\space\space 
V(s)\in R, arbitrarily,for \space all \space s\in S
\\&\space\space\space\space\space\space\space\space 
Returns(s)\leftarrow empty\space list,for \space all \space s\in S
\\
&
Loop \space forever(for \space each \space episode)
\\&\space\space\space\space\space\space\space\space 
Generate\space an \space episode \space following \space \pi:S_0,A_0,R_1,S_1,A_1,R_2,....,S_{T-1},A_{T-1},R_T
\\&\space\space\space\space\space\space\space\space 
G\leftarrow 0
\\&\space\space\space\space\space\space\space\space 
Loop \space for \space each \space step \space of \space episode,t=T-1,T-2,...0
\\&\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space 
G\leftarrow \gamma G+R_{t+1}
\\&\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space 
Unless\space S_t \space appears \space in \space S_0,S_1,...,S_{t-1}
\\&\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space 
Append \space G \space to \space Returns(S_t)
\\&\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space \space\space\space\space\space\space\space 
V(S_t) \leftarrow average(Returns(S_t))



\end{aligned}
$$




Each average is itself an unbiased estimate, and the standard deviation of its error falls as 1/ sqrt(n) where n is the number of returns averaged



<img src="强化学习.assets/image-20210224040618407.png" alt="image-20210224040618407" style="zoom:40%;" />



note that the computational expense of estimating the value of a single state is independent of the number of states

One can generate many sample episodes starting from the states of interest, averaging returns from only these states, ignoring all others. This is a third advantage Monte Carlo methods can have over DP methods (after the ability to learn from actual experience and from simulated experience).



### 5.2 Monte Carlo Estimation of Action Values

* If a model is not available ,then it is particularly useful to estimate action values rather than state values.
* every-visit MC: estimate the value of a state-action pair as the average of the returns that have followed all the visits to it.
* First-visit MC: averages the returns following the first time in each episode that the state was visited and the action was selected.





### 5.3 Monte Carlo Control

For MC policy iteration, alternate between evaluation and improvement on an episode-by-episode basis.



![image-20210224043136877](强化学习.assets/image-20210224043136877.png)



In MC ES, all the returns for each state-action pair are accumulated and averaged ,irrespective of what policy was in force when they were observed, it cannot converge to any suboptimal policy.

<img src="强化学习.assets/image-20210224044019145.png" alt="image-20210224044019145" style="zoom:40%;" />

### 5.4 Monte Carlo Control without ES





* on-policy: evaluate or improve the policy that is used to make decisions. **MC with ES**
* off-policy: evaluate or improve a policy different from that used to generate the data.


$$
\begin{aligned}
\epsilon -soft policy:
\\
\frac{\epsilon}{|A(s)|} \space for \space non-greedy \space actions
\\
1-\epsilon-\frac{\epsilon}{|A(s)|} \space for \space greedy \space action
\end{aligned}
$$


![image-20210224044739741](强化学习.assets/image-20210224044739741.png)




$$
\begin{aligned}
q_{\pi}(s,\pi'(s))
&=\sum_a\pi'(a|s)q_\pi(s,a)
\\
&=\frac{\epsilon}{|A(s)|} \sum_a q_\pi (s,a)+(1-\epsilon)max_a(q_\pi(s,a))
\\
&\ge \frac{\epsilon}{|A(s)|} \sum_a q_\pi(s,a)+(1-\epsilon)\sum_a \frac{\pi(a|s)-\frac{\pi}{|A(s)|}}{1-\epsilon}q_\pi(s,a)
\\
&=v_\pi(s)
\end{aligned}
\\
notice:\sum_a \frac{\pi(a|s)-\frac{\pi}{|A(s)|}}{1-\epsilon}=1
$$

$$
\begin{aligned}
\overline{v}_*(s)
&=(1-\epsilon)max_a(\overline q_*(s,a))+\frac{\epsilon}{|A(s)|}\sum_a\overline q_*(s,a)
\\
&=(1-\epsilon)max_a \sum_{s',r}p(s',r|s,a)[r+\gamma \overline v_*(s')]+\frac{\epsilon}{|A(s)|}\sum_a\sum_{s',r}p(s',r|s,a)[r+\gamma \overline v_*(s')]

\end{aligned}
$$



$$
\begin{aligned}
\overline{v}_\pi(s)
&=(1-\epsilon)max_a \sum_{s',r}p(s',r|s,a)[r+\gamma \overline v_\pi(s')]+\frac{\epsilon}{|A(s)|}\sum_a\sum_{s',r}p(s',r|s,a)[r+\gamma \overline v_\pi(s')]

\end{aligned}
$$


### 5.5 off-policy Prediction via Importance Sampling

* dilemma

  They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions).

* on-policy: simpler 

* off-policy: require additional concepts and notation. Of greater variance and are slower to converge.





Probability of the subsequent state-action trajectory
$$
\begin{aligned}
Pr(A_t,S_{t+1},A_{t+1},...,S_T|S_t,A_{t:T-1}- \pi)
&=\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})...p(S_T|S_{T-1},A_{T-1})
\\
&=\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)

\end{aligned}
$$


**Importance sampling ratio** : weighting returns according to the relative probability of their trajectories occurring under the target and behavior polices
$$
\rho_{t:T-1}=\frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}

{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)}
=\frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)}
$$




**weighted importance sampling**

<img src="强化学习.assets/image-20210224052816784.png" alt="image-20210224052816784" style="zoom:40%;" />



![image-20210224053026251](强化学习.assets/image-20210224053026251.png)


$$
|α \alphaα|$\alpha∣ ∣ | |∣∣\beta$|$\beta∣ ∣ | |∣∣\gamma$|$\gamma∣ ∣ | |∣∣\Gamma$|$\Gamma∣ ∣ | |∣∣\delta$|$\delta∣ ∣ | |∣∣\Delta$|$\Delta∣ ∣ | |∣∣\epsilon$|$\epsilon∣ ∣ | |∣∣\varepsilon$|$\varepsilon∣ ∣ | |∣∣\zeta$|$\zeta∣ ∣ | |∣∣\eta$|$\eta∣ ∣ | |∣∣\theta$|$\theta∣ ∣ | |∣∣\Theta$|$\Theta∣ ∣ | |∣∣\vartheta$|$\vartheta∣ ∣ | |∣∣\iota$|$\iota∣ ∣ | |∣∣\kappa$|$\kappa∣ ∣ | |∣∣\lambda$|$\lambda∣ ∣ | |∣∣\Lambda$|$\Lambda∣ ∣ | |∣∣\mu$|$\mu∣ ∣ | |∣∣\nu$|$\nu∣ ∣ | |∣∣\xi$|$\xi∣ ∣ | |∣∣\Xi$|$\Xi∣ ∣ | |∣∣\pi$|$\pi∣ ∣ | |∣∣\Pi$|$\Pi∣ ∣ | |∣∣\varpi$|$\varpi∣ ∣ | |∣∣\rho$|$\rho∣ ∣ | |∣∣\varrho$|$\varrho∣ ∣ | |∣∣\sigma$|$\sigma∣ ∣ | |∣∣\Sigma$|$\Sigma∣ ∣ | |∣∣\varsigma$|$\varsigma∣ ∣ | |∣∣\tau$|$\tau∣ ∣ | |∣∣\upsilon$|$\upsilon∣ ∣ | |∣∣\Upsilon$|$\Upsilon∣ ∣ | |∣∣\phi$|$\phi∣ ∣ | |∣∣\Phi$|$\Phi∣ ∣ | |∣∣\varphi$|$\varphi∣ ∣ | |∣∣\chi$|$\chi∣ ∣ | |∣∣\psi$|$\psi∣ ∣ | |∣∣\Psi$|$\Psi∣ ∣ | |∣∣\Omega$|$\Omega∣ ∣ | |∣∣\omega$|$\omega$|
$$
<img src="强化学习.assets/image-20210224053118313.png" alt="image-20210224053118313" style="zoom:50%;" />

![image-20210224053259763](强化学习.assets/image-20210224053259763.png)

<img src="强化学习.assets/image-20210224053227316.png" alt="image-20210224053227316" style="zoom:40%;" />













































































































































# Markdown 文档数学公式集合_Martian-CSDN 博客



正文 (inline) 中的 LaTeX 公式用 $…$ 定义，单独一行用 $. . . . ........$ 定义



**1. 上下标**
^ 表示上标，如果是多个字符，请使用 {}，_表示下标，如果是多个字符，请使用 ()



```
$$ f(x) = x_i + y^x，f((x+y)^{u}) = (x+y)^{u+v}$$
```



f (x) = x i + y x ， f ( ( x + y ) u ) = ( x + y ) u + v f(x) = x_i + y^x，f((x+y)^{u}) = (x+y)^{u+v}*f*(*x*)=*x**i*+*y**x*，*f*((*x*+*y*)*u*)=(*x*+*y*)*u*+*v*



**2. 开方**
sqrt 表示开方



```
$\sqrt[m]{n}$ # 表示对n开m次方
$$\sqrt{25}$$
$$\sqrt[3]{25}$$
```



25 ， 25 3 \sqrt{25}， \sqrt[3]{25}25，325



**3. 省略号**
\ldots 表示语文本底线对其的省略号，\cdots 表示与文本中线对其的省略号，\cdot 表示一个点，也就是点乘号



```
这里是省略号$\ldots$，底线对齐
这里是省略号$\cdots$，中线对齐
$$ f(x_1,x_2,\ldots,x_n) = x_1^2 +x_2^2+\cdots+x_n^2 $$
```



这里是省略号… \ldots…，底线对齐
这里是省略号⋯ \cdots⋯，中线对齐
f ( x 1 , x 2 , … , x n ) = x 1 2 + x 2 2 + ⋯ + x n 2 f(x_1,x_2,\ldots,x_n) = x_1^2 +x_2^2+\cdots+x_n^2*f*(*x*1​,*x*2​,…,*x**n*​)=*x*12​+*x*22​+⋯+*x**n*2​



**4. 括号**
(), [] 直接写即可，{} 需要转义



```
这是小括号$()$
这是小括号$[]$
这是小括号$\{\}$
```



( ) , [ ] , { } (), [], \{\}(),[],{}



**5. 分号**
使用 \frac{分子}{分母}，或者使用 分子 \over 分母



```
$$\frac{a+b}{c+d}$$
```



a + b c + d \frac{a+b}{c+d}*c*+*d**a*+*b*,n + m m ⋅ n {n+m} \over {m\cdot n}*m*⋅*n**n*+*m*



**6. 向量**



```
\vec a \cdot \vec b = 0
```



a ⃗ ⋅ b ⃗ = 0 \vec a \cdot \vec b = 0*a*⋅*b*=0



**7. 箭头**



| 示例 | 代码                | 示例 | 代码                |
| ---- | ------------------- | ---- | ------------------- |
| ←    | \leftarrow          | ⇐    | \Leftarrow          |
| →    | \rightarrow         | ⇒    | \Rightarrow         |
| ↔    | \leftrightarrow     | ⇔    | \Leftrightarrow     |
| ⟵    | \longleftarrow      | ⟸    | \Longleftarrow      |
| ⟶    | \longrightarrow     | ⟹    | \Longrightarrow     |
| ⟷    | \longleftrightarrow | ⟺    | \Longleftrightarrow |
| ↑    | \uparrow            | ↓    | \downarrow          |
| ↕    | \updownarrow        |      |                     |



**8. 求和, 求积**



```
$$\sum_{i=1}^{n} x_i$$
$$\prod_{i=0}^n{1 \over {x^2}}$$
```



∑ i = 1 n x i , ∏ i = 0 n 1 x 2 \sum_{i=1}^{n} x_i, \prod_{i=0}^n{1 \over {x^2}}*i*=1∑*n**x**i*,*i*=0∏*n**x*21



**9. 积分**



```
$$\int_0^\infty{f(x)dx}$$
```



∫ 0 ∞ f (x) d x \int_0^\infty{f(x)dx}∫0∞*f*(*x*)*d**x*



**10. 方程式**



```
$$\begin{cases}
a_1x+b_1y+c_1z=d_1\\
a_2x+b_2y+c_2z=d_2\\
a_3x+b_3y+c_3z=d_3\\
\end{cases}
$$
```



{ a 1 x + b 1 y + c 1 z = d 1 a 2 x + b 2 y + c 2 z = d 2 a 3 x + b 3 y + c 3 z = d 3

⎧⎩⎨a1x+b1y+c1z=d1a2x+b2y+c2z=d2a3x+b3y+c3z=d3{a1x+b1y+c1z=d1a2x+b2y+c2z=d2a3x+b3y+c3z=d3



⎩⎪⎨⎪⎧a1x+b1y+c1z=d1a2x+b2y+c2z=d2a3x+b3y+c3z=d3



**11. 矩阵**



```
$$\begin{bmatrix}
{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\
\end{bmatrix}$$
```



[ a 11 a 12 ⋯ a 1 n a 21 a 22 ⋯ a 2 n ⋮ ⋮ ⋱ ⋮ a m 1 a m 2 ⋯ a m n ]

⎡⎣⎢⎢⎢⎢⎢a11a21⋮am1amp;a12amp;a22amp;⋮amp;am2amp;⋯amp;⋯amp;⋱amp;⋯amp;a1namp;a2namp;⋮amp;amn⎤⎦⎥⎥⎥⎥⎥[a11amp;a12amp;⋯amp;a1na21amp;a22amp;⋯amp;a2n⋮amp;⋮amp;⋱amp;⋮am1amp;am2amp;⋯amp;amn]



⎣⎢⎢⎢⎡a11a21⋮am1a12a22⋮am2⋯⋯⋱⋯a1na2n⋮amn⎦⎥⎥⎥⎤



**12. 希腊字符**



| 字符 | 表达式   | 字符 | 表达式 |
| ---- | -------- | ---- | ------ |
| α    | \alpha   | β    | \beta  |
| γ    | \gamma   | δ    | \delta |
| ε    | \epsilon | ζ    | \zeta  |
| η    | \eta     | θ    | \theta |
| ι    | \iota    | κ    | \kappa |
| λ    | \lambda  | μ    | \mu    |
| ν    | \nu      | ξ    | \xi    |
| π    | \pi      | ρ    | \rho   |
| σ    | \sigma   | τ    | \tau   |
| υ    | \upsilon | φ    | \phi   |
| χ    | \chi     | ψ    | \psi   |
| ω    | \omega   |      |        |



**13. 连线**



```
\overline{a+b+c+d}

\underline{a+b+c+d}

\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}

\hat{y} \check{y} \breve{y}
```



a + b + c + d ‾ , a + b + c + d ‾ , a + b + c ⎵ 1.0 + d ⏞ 2.0 , y ^ y ˇ y ˘ \overline{a+b+c+d} , \underline{a+b+c+d}, \overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0} , \hat{y} \check{y} \breve{y}*a*+*b*+*c*+*d*,*a*+*b*+*c*+*d*,*a*+1.0*b*+*c*+*d*2.0,*y*^*y*ˇ*y*˘



**14. 集合运算符号**



```
\emptyset \in \notin \subset \supset \subseteq \supseteq \bigcap \bigcup \bigvee \bigwedge \biguplus \bigsqcup
```



∅ ∈ ∉ ⊂ ⊃ ⊆ ⊇ ⋂ ⋃ ⋁ ⋀ ⨄ ⨆ \emptyset \in \notin \subset \supset \subseteq \supseteq \bigcap \bigcup \bigvee \bigwedge \biguplus \bigsqcup∅∈∈/⊂⊃⊆⊇⋂⋃⋁⋀⨄⨆



**15. 算术运算符号**



```
\pm \times \div \mid
```



± × ÷ ∣ \pm \times \div \mid±×÷∣



**16. 极限符号**



```
$$\lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}$$
```



lim ⁡ n → + ∞ 1 n ( n + 1 ) \lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}*n*→+∞lim*n*(*n*+1)1



**17. 对数函数符号**



| 示例   | 代码      |
| ------ | --------- |
| ln15   | \ln15     |
| log210 | \log_2 10 |
| lg7    | \lg7      |



**18. 三角函数符号**



| 示例 | 代码     |
| ---- | -------- |
| sin  | \sin     |
| cos  | \cos     |
| cot  | \cot     |
| tan  | \tan     |
| csc  | \csc     |
| sec  | \sec     |
| 30∘  | 30^\circ |



**19. 逻辑运算符**



| 代码        | 示例            |
| ----------- | --------------- |
| \because    | ∵               |
| \therefore  | ∴               |
| \forall     | ∀               |
| \not=       | ≠               |
| \exists     | ∃               |
| \not>       | ̸ &gt; \not&gt;̸> |
| \not\subset | ⊄               |

$$
\begin{aligned}
a  &= b^2 + c^2\\
&= w^3 + b
\end{aligned}
$$













































